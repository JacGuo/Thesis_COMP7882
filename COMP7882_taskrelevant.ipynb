{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyP20QZuf6AKBJUmNFfta/i+",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JacGuo/Thesis_COMP7882/blob/main/COMP7882_taskrelevant.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_78uFUa908Xt",
        "outputId": "cb658400-cd6e-4b4b-e7d1-c86713c5512e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.4.0+cu121\n"
          ]
        }
      ],
      "source": [
        "# import pytorch libraries\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader, random_split\n",
        "\n",
        "print(torch.__version__)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn.functional as F\n",
        "from tqdm import tqdm\n",
        "import torchvision.models as models\n",
        "from torchvision.models import ResNet18_Weights"
      ],
      "metadata": {
        "id": "YMlKhnGRv3Mq"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check if CUDA is available and if device is GPU\n",
        "print('Cuda Available : {}'.format(torch.cuda.is_available()))\n",
        "print('GPU - {0}'.format(torch.cuda.get_device_name()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yu_5t61J1d8z",
        "outputId": "8440d8f7-e0e5-4776-9954-40f89e1b40c8"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cuda Available : True\n",
            "GPU - Tesla T4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\")"
      ],
      "metadata": {
        "id": "OIEcetX5wQRb"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Load CIFAR-10 dataset**"
      ],
      "metadata": {
        "id": "P9qUEsYPtHhn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Check if CUDA is available and set the device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print('Using device:', device)\n",
        "\n",
        "# Define transformations for the CIFAR-10 dataset with proper normalization\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))  # CIFAR-10 specific normalization\n",
        "])\n",
        "\n",
        "# Load the CIFAR-10 dataset\n",
        "train_dataset = datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
        "test_dataset = datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
        "\n",
        "# Split the training dataset into training and validation sets\n",
        "train_size = int(0.8 * len(train_dataset))\n",
        "val_size = len(train_dataset) - train_size\n",
        "train_dataset, val_dataset = random_split(train_dataset, [train_size, val_size])\n",
        "\n",
        "# Define the mapping from CIFAR-10 labels to binary labels\n",
        "def map_labels_to_binary(target):\n",
        "    animal_classes = [2, 3, 4, 5, 6, 7]  # 'bird', 'cat', 'deer', 'dog', 'frog', 'horse'\n",
        "    if target in animal_classes:\n",
        "        return 0  # Animal\n",
        "    else:\n",
        "        return 1  # Vehicle\n",
        "\n",
        "# Map the datasets to binary labels\n",
        "train_dataset_binary = [(data, map_labels_to_binary(target)) for data, target in train_dataset]\n",
        "val_dataset_binary = [(data, map_labels_to_binary(target)) for data, target in val_dataset]\n",
        "test_dataset_binary = [(data, map_labels_to_binary(target)) for data, target in test_dataset]\n",
        "\n",
        "# Create data loaders\n",
        "train_loader_binary = DataLoader(train_dataset_binary, batch_size=64, shuffle=True)\n",
        "val_loader_binary = DataLoader(val_dataset_binary, batch_size=64, shuffle=False)\n",
        "test_loader_binary = DataLoader(test_dataset_binary, batch_size=64, shuffle=False)\n",
        "\n",
        "train_loader_multiclass = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
        "val_loader_multiclass = DataLoader(val_dataset, batch_size=64, shuffle=False)\n",
        "test_loader_multiclass = DataLoader(test_dataset, batch_size=64, shuffle=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VxFCxS7qNgzE",
        "outputId": "987364ef-b396-4edc-fd97-e5af640a52f9"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n",
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 170498071/170498071 [00:05<00:00, 30987531.60it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/cifar-10-python.tar.gz to ./data\n",
            "Files already downloaded and verified\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define **Feature Extractor**"
      ],
      "metadata": {
        "id": "j2hv5X3Tu81U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class FeatureExtractor(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(FeatureExtractor, self).__init__()\n",
        "        # Use ResNet-18 with the updated method for loading pretrained weights\n",
        "        self.resnet = models.resnet18(weights=ResNet18_Weights.IMAGENET1K_V1)\n",
        "        # Remove the fully connected layer (fc) from ResNet-18\n",
        "        self.resnet = nn.Sequential(*list(self.resnet.children())[:-1])\n",
        "        # Define a single fully connected layer to output features\n",
        "        self.fc = nn.Linear(512, 256)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Pass input through ResNet-18 (excluding the final layer)\n",
        "        x = self.resnet(x)\n",
        "        x = torch.flatten(x, 1)  # Flatten the output\n",
        "        # Output features directly\n",
        "        features = self.fc(x)\n",
        "        return features\n",
        "\n",
        "\n",
        "def kl_divergence(features):\n",
        "    # Implement a KL divergence based on the feature distribution\n",
        "    mean = torch.mean(features, dim=0)\n",
        "    std = torch.std(features, dim=0)\n",
        "    kl_div = torch.sum(mean**2 + std**2 - torch.log(std**2) - 1)\n",
        "    return kl_div\n"
      ],
      "metadata": {
        "id": "SiRM5r-TvFBn"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define the Target Model - Binary Classification"
      ],
      "metadata": {
        "id": "3ggYjPElv8LM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TargetModelBinary(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(TargetModelBinary, self).__init__()\n",
        "        self.fc1 = nn.Linear(256, 128)\n",
        "        self.fc2 = nn.Linear(128, 64)\n",
        "        self.fc3 = nn.Linear(64, 1)  # Binary classification output\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        x = torch.sigmoid(self.fc3(x))  # Sigmoid for binary classification\n",
        "        return x"
      ],
      "metadata": {
        "id": "1VPsk8EFPJOM"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define Multiclass Image Classification (10 classes)"
      ],
      "metadata": {
        "id": "XE3EnCtWwaps"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class DifferentTaskModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(DifferentTaskModel, self).__init__()\n",
        "        self.fc1 = nn.Linear(256, 128)\n",
        "        self.fc2 = nn.Linear(128, 64)\n",
        "        self.fc3 = nn.Linear(64, 10)  # Output layer for 10 classes\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        x = self.fc3(x)  # No sigmoid or softmax here, as CrossEntropyLoss expects logits\n",
        "        return x"
      ],
      "metadata": {
        "id": "hP5_l0HPwgJS"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Training**"
      ],
      "metadata": {
        "id": "Nz5Xv_xF4lgn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create the model instance"
      ],
      "metadata": {
        "id": "PKbCfpGooPnQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Instantiate models and move them to the device\n",
        "feature_extractor = FeatureExtractor().to(device)\n",
        "target_model_binary = TargetModelBinary().to(device)\n",
        "different_task_model = DifferentTaskModel().to(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ana2-DN1oL6W",
        "outputId": "3db6cdba-134d-4548-fe73-d0e06b566e4f"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to /root/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth\n",
            "100%|██████████| 44.7M/44.7M [00:00<00:00, 150MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define the loss function and optimizer"
      ],
      "metadata": {
        "id": "hRv3iuOSiGW1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the binary cross-entropy loss for binary classification\n",
        "criterion_binary = nn.BCELoss()\n",
        "\n",
        "# Define the cross-entropy loss for 10-class classification\n",
        "criterion_multiclass = nn.CrossEntropyLoss()\n",
        "\n",
        "# Define the overall loss function for binary classification\n",
        "def total_loss_binary(features, task_output, target, lambda_val=0.01):\n",
        "    kl_loss = kl_divergence(features)\n",
        "    task_loss = criterion_binary(task_output, target.float().view(-1, 1))\n",
        "    return task_loss + lambda_val * kl_loss\n",
        "\n",
        "\n",
        "\n",
        "# Define optimizers\n",
        "optimizer_binary = optim.Adam(list(feature_extractor.parameters()) + list(target_model_binary.parameters()), lr=1e-4)\n",
        "\n"
      ],
      "metadata": {
        "id": "IF2ANI5MsQPJ"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Training loop"
      ],
      "metadata": {
        "id": "AsANQQOjkkXs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Number of epochs to train the model\n",
        "num_epochs = 10\n",
        "\n",
        "# Training loop for binary classification\n",
        "for epoch in range(num_epochs):\n",
        "    feature_extractor.train()\n",
        "    target_model_binary.train()\n",
        "\n",
        "    running_loss = 0.0\n",
        "\n",
        "    for data, targets in tqdm(train_loader_binary, desc=f\"Epoch {epoch+1} - Binary Task\"):\n",
        "        data, targets = data.to(device), targets.to(device)\n",
        "\n",
        "        # Zero the parameter gradients\n",
        "        optimizer_binary.zero_grad()\n",
        "\n",
        "        # Forward pass\n",
        "        features = feature_extractor(data)\n",
        "        outputs = target_model_binary(features)\n",
        "\n",
        "        # Compute the loss\n",
        "        loss = total_loss_binary(features, outputs, targets)\n",
        "\n",
        "        # Backward pass and optimize\n",
        "        loss.backward()\n",
        "        optimizer_binary.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "\n",
        "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {running_loss/len(train_loader_binary):.4f}\")\n",
        "\n",
        "\n",
        "# Validate the binary classification task\n",
        "feature_extractor.eval()\n",
        "target_model_binary.eval()\n",
        "correct = 0\n",
        "total = 0\n",
        "with torch.no_grad():\n",
        "    for data, targets in val_loader_binary:\n",
        "        data, targets = data.to(device), targets.to(device)\n",
        "        features = feature_extractor(data)\n",
        "        outputs = target_model_binary(features)\n",
        "        predicted = (outputs > 0.5).float()  # Apply threshold for binary classification\n",
        "        total += targets.size(0)\n",
        "        correct += (predicted.view(-1) == targets).sum().item()\n",
        "\n",
        "val_accuracy_binary = correct * 100 / total\n",
        "print(f'Validation Accuracy (Binary Task): {val_accuracy_binary:.2f}%')\n",
        "\n"
      ],
      "metadata": {
        "id": "PBOEnchV4nXF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "89cdf2a6-21e0-49aa-f31d-9bde94eca51c"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1 - Binary Task: 100%|██████████| 625/625 [00:17<00:00, 36.74it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/10], Loss: 0.2506\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 2 - Binary Task: 100%|██████████| 625/625 [00:16<00:00, 38.04it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [2/10], Loss: 0.0898\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 3 - Binary Task: 100%|██████████| 625/625 [00:15<00:00, 41.42it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [3/10], Loss: 0.0525\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 4 - Binary Task: 100%|██████████| 625/625 [00:15<00:00, 40.52it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [4/10], Loss: 0.0348\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 5 - Binary Task: 100%|██████████| 625/625 [00:14<00:00, 42.03it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [5/10], Loss: 0.0257\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 6 - Binary Task: 100%|██████████| 625/625 [00:14<00:00, 42.53it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [6/10], Loss: 0.0204\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 7 - Binary Task: 100%|██████████| 625/625 [00:14<00:00, 43.03it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [7/10], Loss: 0.0154\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 8 - Binary Task: 100%|██████████| 625/625 [00:14<00:00, 43.13it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [8/10], Loss: 0.0162\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 9 - Binary Task: 100%|██████████| 625/625 [00:14<00:00, 42.51it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [9/10], Loss: 0.0144\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 10 - Binary Task: 100%|██████████| 625/625 [00:14<00:00, 42.36it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [10/10], Loss: 0.0117\n",
            "Validation Accuracy (Binary Task): 96.99%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Test feature extractor using multiclass classification task"
      ],
      "metadata": {
        "id": "gEWiY6VNlDne"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Test the feature extractor on the multiclass classification task\n",
        "feature_extractor.eval()  # Feature extractor is frozen (not updated)\n",
        "different_task_model.train()  # Only train the different task model\n",
        "\n",
        "optimizer_multiclass = optim.Adam(different_task_model.parameters(), lr=1e-4)\n",
        "\n",
        "# Train the DifferentTaskModel on the features for the 10-class task\n",
        "for epoch in range(num_epochs):\n",
        "    running_loss = 0\n",
        "    for data, targets in tqdm(train_loader_multiclass, desc=f\"Epoch {epoch+1} - Multiclass Task\"):\n",
        "        data, targets = data.to(device), targets.to(device)\n",
        "\n",
        "        # Zero the parameter gradients\n",
        "        optimizer_multiclass.zero_grad()\n",
        "\n",
        "        # Forward pass through the feature extractor (frozen) and different task model\n",
        "        with torch.no_grad():  # Do not update the feature extractor\n",
        "            features = feature_extractor(data)\n",
        "        outputs = different_task_model(features)\n",
        "\n",
        "        # Compute the loss\n",
        "        loss = criterion_multiclass(outputs, targets)\n",
        "\n",
        "        # Backward pass and optimize (only updates the different task model)\n",
        "        loss.backward()\n",
        "        optimizer_multiclass.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "\n",
        "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {running_loss/len(train_loader_multiclass):.4f}\")\n",
        "\n",
        "# Evaluate the DifferentTaskModel on the test data\n",
        "different_task_model.eval()\n",
        "correct = 0\n",
        "total = 0\n",
        "with torch.no_grad():\n",
        "    for data, targets in test_loader_multiclass:\n",
        "        data, targets = data.to(device), targets.to(device)\n",
        "        features = feature_extractor(data)  # Features from the frozen feature extractor\n",
        "        outputs = different_task_model(features)\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "        total += targets.size(0)\n",
        "        correct += (predicted == targets).sum().item()\n",
        "\n",
        "test_accuracy_multiclass = correct * 100 / total\n",
        "print(f'Test Accuracy (Multiclass Task): {test_accuracy_multiclass:.2f}%')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JrDrO04glCGr",
        "outputId": "ada701aa-a8ac-485d-98f6-ba4dbd0c7c74"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1 - Multiclass Task: 100%|██████████| 625/625 [00:14<00:00, 42.63it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/10], Loss: 1.6661\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 2 - Multiclass Task: 100%|██████████| 625/625 [00:14<00:00, 42.82it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [2/10], Loss: 1.4302\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 3 - Multiclass Task: 100%|██████████| 625/625 [00:14<00:00, 42.69it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [3/10], Loss: 1.3073\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 4 - Multiclass Task: 100%|██████████| 625/625 [00:15<00:00, 41.14it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [4/10], Loss: 1.2266\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 5 - Multiclass Task: 100%|██████████| 625/625 [00:16<00:00, 38.43it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [5/10], Loss: 1.1742\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 6 - Multiclass Task: 100%|██████████| 625/625 [00:14<00:00, 42.30it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [6/10], Loss: 1.1368\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 7 - Multiclass Task: 100%|██████████| 625/625 [00:14<00:00, 42.26it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [7/10], Loss: 1.1097\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 8 - Multiclass Task: 100%|██████████| 625/625 [00:15<00:00, 41.04it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [8/10], Loss: 1.0913\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 9 - Multiclass Task: 100%|██████████| 625/625 [00:14<00:00, 41.83it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [9/10], Loss: 1.0763\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 10 - Multiclass Task: 100%|██████████| 625/625 [00:15<00:00, 41.58it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [10/10], Loss: 1.0640\n",
            "Test Accuracy (Multiclass Task): 56.13%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Test the binary classification task on the test set\n",
        "correct = 0\n",
        "total = 0\n",
        "with torch.no_grad():\n",
        "    for data, targets in test_loader_binary:\n",
        "        data, targets = data.to(device), targets.to(device)\n",
        "        features = feature_extractor(data)\n",
        "        outputs = target_model_binary(features)\n",
        "        predicted = (outputs > 0.5).float()  # Apply threshold for binary classification\n",
        "        total += targets.size(0)\n",
        "        correct += (predicted.view(-1) == targets).sum().item()\n",
        "\n",
        "test_accuracy_binary = correct * 100 / total\n",
        "print(f'Test Accuracy (Binary Task): {test_accuracy_binary:.2f}%')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UHlxeDPdsFPf",
        "outputId": "ddbd1c82-daa4-4e21-d311-e39425cf9fa8"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Accuracy (Binary Task): 96.59%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class EnhancedDifferentTaskModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(EnhancedDifferentTaskModel, self).__init__()\n",
        "        # Fully connected layers with an additional layer\n",
        "        self.fc1 = nn.Linear(256, 512)  # Input is 256, as provided by the feature extractor\n",
        "        self.fc2 = nn.Linear(512, 256)  # New additional layer\n",
        "        self.fc3 = nn.Linear(256, 128)\n",
        "        self.fc4 = nn.Linear(128, 10)  # Output layer for 10 classes\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.relu(self.fc2(x))  # New additional layer\n",
        "        x = F.relu(self.fc3(x))\n",
        "        x = self.fc4(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "Brv0KvQiY82e"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Instantiate the enhanced multiclass model and move it to the device\n",
        "enhanced_multiclass_model = EnhancedDifferentTaskModel().to(device)\n",
        "\n",
        "# Use Adam optimizer for the enhanced multiclass model\n",
        "optimizer_multiclass = optim.Adam(enhanced_multiclass_model.parameters(), lr=1e-4)\n",
        "\n",
        "# Use CrossEntropyLoss for multiclass classification\n",
        "criterion_multiclass = nn.CrossEntropyLoss()\n",
        "\n",
        "# Number of epochs to train the model\n",
        "num_epochs = 10\n",
        "\n",
        "# Training loop for multiclass classification\n",
        "for epoch in range(num_epochs):\n",
        "    enhanced_multiclass_model.train()\n",
        "\n",
        "    running_loss = 0.0\n",
        "\n",
        "    for data, targets in tqdm(train_loader_multiclass, desc=f\"Epoch {epoch+1} - Multiclass Task\"):\n",
        "        data, targets = data.to(device), targets.to(device)\n",
        "\n",
        "        # Zero the parameter gradients\n",
        "        optimizer_multiclass.zero_grad()\n",
        "\n",
        "        # Forward pass\n",
        "        features = feature_extractor(data)  # Use the feature extractor to get the features\n",
        "        outputs = enhanced_multiclass_model(features)\n",
        "\n",
        "        # Compute the loss\n",
        "        loss = criterion_multiclass(outputs, targets)\n",
        "\n",
        "        # Backward pass and optimize\n",
        "        loss.backward()\n",
        "        optimizer_multiclass.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "\n",
        "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {running_loss/len(train_loader_multiclass):.4f}\")\n",
        "\n",
        "# Validate the enhanced multiclass classification task\n",
        "enhanced_multiclass_model.eval()\n",
        "correct = 0\n",
        "total = 0\n",
        "with torch.no_grad():\n",
        "    for data, targets in val_loader_multiclass:\n",
        "        data, targets = data.to(device), targets.to(device)\n",
        "        features = feature_extractor(data)\n",
        "        outputs = enhanced_multiclass_model(features)\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "        total += targets.size(0)\n",
        "        correct += (predicted == targets).sum().item()\n",
        "\n",
        "val_accuracy_multiclass = correct * 100 / total\n",
        "print(f'Validation Accuracy (Multiclass Task): {val_accuracy_multiclass:.2f}%')\n",
        "\n",
        "# Test the enhanced multiclass classification task on the test set\n",
        "correct = 0\n",
        "total = 0\n",
        "with torch.no_grad():\n",
        "    for data, targets in test_loader_multiclass:\n",
        "        data, targets = data.to(device), targets.to(device)\n",
        "        features = feature_extractor(data)\n",
        "        outputs = enhanced_multiclass_model(features)\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "        total += targets.size(0)\n",
        "        correct += (predicted == targets).sum().item()\n",
        "\n",
        "test_accuracy_multiclass = correct * 100 / total\n",
        "print(f'Test Accuracy (Multiclass Task): {test_accuracy_multiclass:.2f}%')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dsRyQJY6aYeE",
        "outputId": "4c766e86-12a5-426a-c26d-6c44fce18acf"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1 - Multiclass Task: 100%|██████████| 625/625 [00:21<00:00, 28.72it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/10], Loss: 1.5625\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 2 - Multiclass Task: 100%|██████████| 625/625 [00:20<00:00, 30.17it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [2/10], Loss: 1.3115\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 3 - Multiclass Task: 100%|██████████| 625/625 [00:21<00:00, 29.28it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [3/10], Loss: 1.1788\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 4 - Multiclass Task: 100%|██████████| 625/625 [00:21<00:00, 29.49it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [4/10], Loss: 1.1203\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 5 - Multiclass Task: 100%|██████████| 625/625 [00:20<00:00, 29.89it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [5/10], Loss: 1.0939\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 6 - Multiclass Task: 100%|██████████| 625/625 [00:21<00:00, 29.49it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [6/10], Loss: 1.0720\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 7 - Multiclass Task: 100%|██████████| 625/625 [00:21<00:00, 29.23it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [7/10], Loss: 1.0584\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 8 - Multiclass Task: 100%|██████████| 625/625 [00:21<00:00, 29.56it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [8/10], Loss: 1.0428\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 9 - Multiclass Task: 100%|██████████| 625/625 [00:20<00:00, 30.23it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [9/10], Loss: 1.0304\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 10 - Multiclass Task: 100%|██████████| 625/625 [00:21<00:00, 29.48it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [10/10], Loss: 1.0202\n",
            "Validation Accuracy (Multiclass Task): 57.40%\n",
            "Test Accuracy (Multiclass Task): 57.79%\n"
          ]
        }
      ]
    }
  ]
}